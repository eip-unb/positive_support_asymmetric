{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f435e7-d7c0-4268-a500-e4bb4029ec79",
   "metadata": {},
   "source": [
    "# ECDF Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c61a134-e5cf-46b6-b259-b54e861a4d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiago/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import weibull_min,gamma, gengamma, invgamma,halfnorm,halfgennorm,rayleigh,erlang\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import genextreme\n",
    "from scipy import integrate\n",
    "import matplotlib.pyplot as plt\n",
    "from pynverse import inversefunc\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import uniform\n",
    "from scipy.optimize import root_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e808e859-d0e2-4e37-821f-e6737bbf9f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#density function\n",
    "#y: data set\n",
    "def dg(y, theta1, theta2, theta3, theta4, theta5, theta6):\n",
    "    c_theta = NumIntH(theta1, theta2, theta3, theta4, theta5, theta6)\n",
    "    return (1./c_theta)*(y**theta6)*np.exp(-theta1*y-((theta2*y**theta3)+theta4)**theta5)\n",
    "\n",
    "#log-likelihood function\n",
    "#x0: data set\n",
    "def llf(x, t1, t2, t3, t4, t5, t6):\n",
    "\n",
    "    x0=x[dg(x, t1, t2, t3, t4, t5, t6)>0]\n",
    "\n",
    "    return (sum(np.log(dg(x0, t1, t2, t3, t4, t5, t6))))\n",
    "\n",
    "def eval_fit_extreme(samples, parameters_tuple, no_print=True):\n",
    "        \n",
    "    logLik = llf(samples, parameters_tuple[0], parameters_tuple[1], parameters_tuple[2], parameters_tuple[3], parameters_tuple[4], parameters_tuple[5])\n",
    "\n",
    "    k = len(parameters_tuple)\n",
    "    aic = 2*k - 2*(logLik)\n",
    "    n = np.log(len(samples))\n",
    "    aicc = aic + 2*k*(k+1)/(n-k-1)\n",
    "\n",
    "    bic = k*n-2*logLik\n",
    "    edc = -2*logLik-k*np.log(n)\n",
    "\n",
    "    if(no_print):\n",
    "        print('AIC: ', aic, 'AICc:', aicc, 'BIC: ', bic, 'EDC:', edc,'Log-Likelihood', logLik)\n",
    "    \n",
    "    return aic, aicc, bic, edc, logLik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff98808-a7ef-4ef2-9e97-168596cb36b4",
   "metadata": {},
   "source": [
    "# Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b849b6ab-d3a8-4e4b-a254-876cb66d045c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n"
     ]
    }
   ],
   "source": [
    "X = [620,470,260,89,388,242, 103,100,39,460,284,1285, 218,393,106,158,152,477, 403,103,69,158,818,947, 399,1274,32,12,134,660, 548,381,203,871,193,531, 317,85,1410,250,41,1101, 32,421,32,343,376,1512, 1792,47,95,76,515,72, 1585,253,6,860,89,1055, 537,101,385,176,11,565, 164,16,1267,352,160,195, 1279,356,751,500,803,560, 151,24,689,1119,1733,2194, 763,555,14,45,776,1, 1747,945,12,1453,14,150, 20,41,35,69,195,89, 1090,1868,294,96,618,44, 142,892,1307,310,230,30, 403,860,23,406,1054,1935, 561,348,130,13,230,250, 317,304,79,1793,536,12, 9,256,201,733,510,660, 122,27,273,1231,182,289, 667,761,1096,43,44,87, 405,998,1409,61,278,407, 113,25,940,28,848,41, 646,575,219,303,304,38, 195,1061,174,377,388,10, 246,323,198,234,39,308, 55,729,813,1216,1618,539, 6,1566,459,946,764,794, 35,181,147,116,141,19, 380,609,546]\n",
    "\n",
    "print(len(X))\n",
    "\n",
    "data2 = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0ba41ee-ed1f-43d3-a20a-5217486eb3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def expandgrid(*itrs): # https://stackoverflow.com/a/12131385/1100107\n",
    "    \"\"\"\n",
    "    Cartesian product. Reversion is for compatibility with R.\n",
    "    \n",
    "    \"\"\"\n",
    "    product = list(itertools.product(*reversed(itrs)))\n",
    "    return [[x[i] for x in product] for i in range(len(itrs))][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1240377-969f-40ad-8869-218ad4c5eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, early_stop \n",
    "\n",
    "space = {\"t1\":hp.uniform(\"t1\", 0, 10),\n",
    "    \"t2\":hp.uniform(\"t2\", 0, 10),\n",
    "    \"t3\":hp.uniform(\"t3\", -10, 10),\n",
    "    \"t4\":hp.uniform(\"t4\", 0, 10),\n",
    "    \"t5\":hp.uniform(\"t5\", -10, 10),\n",
    "    \"t6\":hp.uniform(\"t6\", 0, 50)\n",
    "}\n",
    "\n",
    "from hyperopt.fmin import generate_trials_to_calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d59acc7-a9aa-420c-883c-29d60b84b943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.045], [1], [-1.284], [0], [5], [0], [50]]\n",
      "1 7\n",
      "0.045\n",
      "1\n",
      "-1.284\n",
      "0\n",
      "5\n",
      "0\n",
      "50\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a Tuple\n",
    "a1 = [0.045]\n",
    "a2 = [1]\n",
    "a3 = [-1.284]\n",
    "a4 = [0]\n",
    "a5 = [5]\n",
    "a6 = [0]\n",
    "\n",
    "\n",
    "n = [50]\n",
    "\n",
    "#matriz de parametros\n",
    "m_par = expandgrid(a1, a2, a3, a4, a5, a6, n)\n",
    "\n",
    "print(m_par)\n",
    "\n",
    "print(m_par[1][0], len(m_par))\n",
    "\n",
    "lines = len(m_par[0])\n",
    "columns = len(m_par)\n",
    "\n",
    "for i in range(lines):\n",
    "    for j in range(columns):\n",
    "        print(m_par[j][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19ed475e-15fe-4595-a231-afef2e21591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.integrate import quad\n",
    "\n",
    "# Funções fornecidas\n",
    "def H_teta(a1, a2, a3, a4, a5, a6):\n",
    "    return quad(lambda x: (x**a6)*np.exp(-a1*x - (a2*x**a3 + a4)**a5), 0, np.inf)[0]\n",
    "\n",
    "def pdf_HG(y, par):\n",
    "    t1, t2, t3, t4, t5, t6 = par\n",
    "    H_val = H_teta(t1, t2, t3, t4, t5, t6)\n",
    "    y = np.array(y)\n",
    "    pdfh = 1 / H_val * y**t6 * np.exp(-t1 * y - (t2 * y**t3 + t4)**t5)\n",
    "    return pdfh\n",
    "\n",
    "def NumIntH(a1, a2, a3, a4, a5, a6):\n",
    "    # print('theta6= ', a6)\n",
    "    return quad(lambda x: (x**a6)*np.exp(-a1*x - (a2*x**a3 + a4)**a5), 0, np.inf)[0]\n",
    "\n",
    "def NumIntH_distribution(a1, a2, a3, a4, a5, a6, y):\n",
    "    return quad(lambda x: (x**a6)*np.exp(-a1*x - (a2*x**a3 + a4)**a5), 0, y)[0]\n",
    "\n",
    "def distribution_extreme_H(x, par):\n",
    "    t1, t2, t3, t4, t5, t6 = par\n",
    "    c_theta = NumIntH(t1, t2, t3, t4, t5, t6)\n",
    "   \n",
    "    cdf_vec = [0.]*len(x)\n",
    "    \n",
    "    for index, val in enumerate(x):\n",
    "        if val==0:\n",
    "            cdf_vec[index] = 0\n",
    "        elif np.isnan(val):\n",
    "            cdf_vec[index] = 0\n",
    "        else:\n",
    "            # print('val =', val)\n",
    "            cdf_vec[index] = (1./c_theta) * NumIntH_distribution(t1, t2, t3, t4, t5, t6, val)\n",
    "\n",
    "    return cdf_vec\n",
    "\n",
    "# Função para calcular a ECDF empírica\n",
    "def ecdf(data):\n",
    "    x = np.sort(data)\n",
    "    y = np.arange(1, len(data) + 1) / len(data)\n",
    "    return x, y\n",
    "\n",
    "# Função para calcular o erro entre a ECDF empírica e a CDF teórica\n",
    "def ecdf_error(params, data):\n",
    "    x_empirical, y_empirical = ecdf(data)\n",
    "    y_theoretical = distribution_extreme_H(x_empirical, params)\n",
    "    error = np.sum((y_empirical - y_theoretical) ** 2)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d242ad5-4d2a-443c-b1a7-010b084375e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extreme_H_fit_box(sample_data, space):\n",
    "\n",
    "    global data\n",
    "\n",
    "    data = sample_data\n",
    "    \n",
    "    best = fmin(lambda x: ecdf_error_hyperopt(x),\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=10000, early_stop_fn=early_stop.no_progress_loss(1000) )\n",
    "        # max_evals=10000, early_stop_fn=early_stop.no_progress_loss(1000), trials=trials)\n",
    "\n",
    "    print(best)\n",
    "    \n",
    "    optimal_params = list(best.values())\n",
    "    return optimal_params\n",
    "\n",
    "def extreme_H_fit_box_trials(sample_data, space, trials):\n",
    "\n",
    "    global data\n",
    "\n",
    "    data = sample_data\n",
    "    \n",
    "    best = fmin(lambda x: ecdf_error_hyperopt(x),\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=10000, early_stop_fn=early_stop.no_progress_loss(1000), trials=trials)\n",
    "\n",
    "    print(best)\n",
    "    \n",
    "    optimal_params = list(best.values())\n",
    "    return optimal_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f1770ec-4e94-497b-b7b9-cb81bdae7549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_extreme_H_dict(x, par):\n",
    "    t1, t2, t3, t4, t5, t6 = par\n",
    "    c_theta = NumIntH(t1, t2, t3, t4, t5, t6)\n",
    "\n",
    "    # ATENÇÃO\n",
    "    # o Hyperopt está gerando combinações de parâmetros que levam c_theta==0.\n",
    "    # resolvi explodir o retorno y_theoretical -> inf) para lidar com a exceção.\n",
    "    if(c_theta==0):\n",
    "        # print(par)\n",
    "        return np.inf\n",
    "        \n",
    "    \n",
    "    cdf_vec = [0.]*len(x)\n",
    "    \n",
    "    for index, val in enumerate(x):\n",
    "        cdf_vec[index] = (1./c_theta) * NumIntH_distribution(t1, t2, t3, t4, t5, t6, val)\n",
    "\n",
    "    return cdf_vec\n",
    "\n",
    "# Função para calcular o erro entre a ECDF empírica e a CDF teórica\n",
    "def ecdf_error_dict(params, data):\n",
    "    x_empirical, y_empirical = ecdf(data)\n",
    "    y_theoretical = distribution_extreme_H_dict(x_empirical, params)\n",
    "    error = np.sum((y_empirical - y_theoretical) ** 2)\n",
    "    return error\n",
    "\n",
    "def ecdf_error_hyperopt(dict_ent):\n",
    "    t1 = dict_ent['t1']\n",
    "    t2 = dict_ent['t2']\n",
    "    t3 = dict_ent['t3']\n",
    "    t4 = dict_ent['t4']\n",
    "    t5 = dict_ent['t5']\n",
    "    t6 = dict_ent['t6']\n",
    "    # print(data)\n",
    "    return ecdf_error_dict([t1, t2, t3, t4, t5, t6], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54990633-58d2-4b74-b1c5-918868d8a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, early_stop \n",
    "\n",
    "from hyperopt.fmin import generate_trials_to_calculate\n",
    "\n",
    "data = []\n",
    "\n",
    "def ecdf_error_hyperopt(dict_ent):\n",
    "    t1 = dict_ent['t1']\n",
    "    t2 = dict_ent['t2']\n",
    "    t3 = dict_ent['t3']\n",
    "    t4 = dict_ent['t4']\n",
    "    t5 = dict_ent['t5']\n",
    "    t6 = dict_ent['t6']\n",
    "    # print(data)\n",
    "    return ecdf_error_dict([t1, t2, t3, t4, t5, t6], data)\n",
    "\n",
    "def extreme_H_fit(sample_data):\n",
    "\n",
    "    global data\n",
    "\n",
    "    data = sample_data\n",
    "\n",
    "    # theta4 and theta5 box\n",
    "    space = {\"t1\":hp.uniform(\"t1\", 0, 10),\n",
    "        \"t2\":hp.uniform(\"t2\", 0, 10),\n",
    "        \"t3\":hp.uniform(\"t3\", -10, 10),\n",
    "        \"t4\":hp.uniform(\"t4\", 0, 0.01),\n",
    "        \"t5\":hp.uniform(\"t5\", 0.99, 1.01),\n",
    "        \"t6\":hp.uniform(\"t6\", -10, 10)\n",
    "    }\n",
    "\n",
    "    best = fmin(lambda x: ecdf_error_hyperopt(x),\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=10000, early_stop_fn=early_stop.no_progress_loss(1000) )\n",
    "    \n",
    "    print(best)\n",
    "    \n",
    "    optimal_params = list(best.values())\n",
    "    return optimal_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "134a369e-1401-40d5-a23e-931bbfff0746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "  0%|                                            | 3/10000 [00:00<07:57, 20.94trial/s, best loss: 0.053736574183174596]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75028/656181929.py:19: IntegrationWarning: The algorithm does not converge.  Roundoff error is detected\n",
      "  in the extrapolation table.  It is assumed that the requested tolerance\n",
      "  cannot be achieved, and that the returned result (if full_output = 1) is \n",
      "  the best which can be obtained.\n",
      "  return quad(lambda x: (x**a6)*np.exp(-a1*x - (a2*x**a3 + a4)**a5), 0, np.inf)[0]\n",
      "\n",
      "/tmp/ipykernel_75028/656181929.py:22: IntegrationWarning: The algorithm does not converge.  Roundoff error is detected\n",
      "  in the extrapolation table.  It is assumed that the requested tolerance\n",
      "  cannot be achieved, and that the returned result (if full_output = 1) is \n",
      "  the best which can be obtained.\n",
      "  return quad(lambda x: (x**a6)*np.exp(-a1*x - (a2*x**a3 + a4)**a5), 0, y)[0]\n",
      "\n",
      "/tmp/ipykernel_75028/656181929.py:22: IntegrationWarning: The integral is probably divergent, or slowly convergent.\n",
      "  return quad(lambda x: (x**a6)*np.exp(-a1*x - (a2*x**a3 + a4)**a5), 0, y)[0]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                           | 12/10000 [00:00<11:09, 14.92trial/s, best loss: 0.053736574183174596]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75028/656181929.py:19: IntegrationWarning: The integral is probably divergent, or slowly convergent.\n",
      "  return quad(lambda x: (x**a6)*np.exp(-a1*x - (a2*x**a3 + a4)**a5), 0, np.inf)[0]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|▏                                          | 44/10000 [00:02<09:22, 17.70trial/s, best loss: 0.053736574183174596]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75028/656181929.py:22: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  return quad(lambda x: (x**a6)*np.exp(-a1*x - (a2*x**a3 + a4)**a5), 0, y)[0]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2%|▋                                         | 171/10000 [00:10<13:48, 11.87trial/s, best loss: 0.053736574183174596]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75028/656181929.py:22: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  return quad(lambda x: (x**a6)*np.exp(-a1*x - (a2*x**a3 + a4)**a5), 0, y)[0]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6%|██▌                                       | 615/10000 [00:40<11:26, 13.66trial/s, best loss: 0.053736574183174596]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75028/656181929.py:19: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  return quad(lambda x: (x**a6)*np.exp(-a1*x - (a2*x**a3 + a4)**a5), 0, np.inf)[0]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19%|████████                                  | 1928/10000 [02:10<09:06, 14.78trial/s, best loss: 0.04858418734799191]\n",
      "{'t1': np.float64(0.0013959587495859867), 't2': np.float64(7.524374651079849), 't3': np.float64(2.1376126910902986), 't4': np.float64(0.0013014252196971883), 't5': np.float64(-0.9807469108467597), 't6': np.float64(-0.28607919974964496)}\n",
      "[np.float64(0.0013959587495859867), np.float64(7.524374651079849), np.float64(2.1376126910902986), np.float64(0.0013014252196971883), np.float64(-0.9807469108467597), np.float64(-0.28607919974964496)]\n",
      " 33%|█████████████▌                           | 3323/10000 [02:02<09:45, 11.41trial/s, best loss: 0.023675049361067756]\n",
      "{'t1': np.float64(0.0014600180926046908), 't2': np.float64(7.168558250732656), 't3': np.float64(-6.554152619274005), 't4': np.float64(0.0017861406540565223), 't5': np.float64(8.57484235581811), 't6': np.float64(-0.29244892768030395)}\n",
      "[np.float64(0.0014600180926046908), np.float64(7.168558250732656), np.float64(-6.554152619274005), np.float64(0.0017861406540565223), np.float64(8.57484235581811), np.float64(-0.29244892768030395)]\n"
     ]
    }
   ],
   "source": [
    "rounds = 2\n",
    "\n",
    "lines = 1\n",
    "\n",
    "optimal_params_vec = []\n",
    "\n",
    "\n",
    "for i in range(lines):\n",
    "    a1 = m_par[0][i]\n",
    "    a2 = m_par[1][i]\n",
    "    a3 = m_par[2][i]\n",
    "    a4 = m_par[3][i]\n",
    "    a5 = m_par[4][i]\n",
    "    a6 = m_par[5][i]\n",
    "    n  = m_par[6][i]\n",
    "\n",
    "    print('i =', i)\n",
    "    \n",
    "    sample = data2\n",
    "\n",
    "    # Frechet: fixando theta1 e theta4 (Frechet)\n",
    "    space = {\"t1\":hp.uniform(\"t1\", 0, 0.01),\n",
    "        \"t2\":hp.uniform(\"t2\", 0, 10),\n",
    "        \"t3\":hp.uniform(\"t3\", -10, 10),\n",
    "        \"t4\":hp.uniform(\"t4\", 0, 0.01),\n",
    "        \"t5\":hp.uniform(\"t5\", -10, 10),\n",
    "        \"t6\":hp.uniform(\"t6\", -10, 10)\n",
    "    }\n",
    "\n",
    "    # from Gamma\n",
    "\n",
    "    shape= 0.8274221404636326\n",
    "    beta= 0.0017845959485496782\n",
    "    reference_params = [beta, 0, 1, 0, 1, shape-1]\n",
    "    trials = generate_trials_to_calculate([{'t1' : reference_params[0], 't2' : reference_params[1], 't3' : reference_params[2], 't4' : reference_params[3], 't5': reference_params[4], 't6' : reference_params[5]}])\n",
    "\n",
    "    for j in range(rounds):  \n",
    "        # optimal_params = extreme_H_fit_box(sample, space)\n",
    "        optimal_params = extreme_H_fit_box_trials(sample, space, trials)\n",
    "        optimal_params_vec.append(optimal_params)\n",
    "        print(optimal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbbde378-ae72-499f-a6c0-b09e110a2539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference parameters: [0.0017845959485496782, 0, 1, 0, 1, -0.1725778595363674]\n",
      "Optimal parameters: [np.float64(0.0013959587495859867), np.float64(7.524374651079849), np.float64(2.1376126910902986), np.float64(0.0013014252196971883), np.float64(-0.9807469108467597), np.float64(-0.28607919974964496)]\n",
      "reference (special case)\n",
      "AIC:  2876.8516235519837 AICc: 2827.343612124952 BIC:  2896.6714530003383 EDC: 2854.84164238931 Log-Likelihood -1432.4258117759919\n",
      "optimized\n",
      "AIC:  2878.4856674377747 AICc: 2828.977656010743 BIC:  2898.3054968861293 EDC: 2856.475686275101 Log-Likelihood -1433.2428337188874\n",
      "\n",
      "\n",
      "Reference parameters: [0.0017845959485496782, 0, 1, 0, 1, -0.1725778595363674]\n",
      "Optimal parameters: [np.float64(0.0014600180926046908), np.float64(7.168558250732656), np.float64(-6.554152619274005), np.float64(0.0017861406540565223), np.float64(8.57484235581811), np.float64(-0.29244892768030395)]\n",
      "reference (special case)\n",
      "AIC:  2876.8516235519837 AICc: 2827.343612124952 BIC:  2896.6714530003383 EDC: 2854.84164238931 Log-Likelihood -1432.4258117759919\n",
      "optimized\n",
      "AIC:  2865.101368134772 AICc: 2815.5933567077404 BIC:  2884.9211975831267 EDC: 2843.0913869720985 Log-Likelihood -1426.550684067386\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from Gamma\n",
    "\n",
    "shape= 0.8274221404636326\n",
    "beta= 0.0017845959485496782\n",
    "reference_params = [beta, 0, 1, 0, 1, shape-1]\n",
    "\n",
    "# optimized\n",
    "# reference_params = [1.38873405e-03, 2.03562041e+00, 4.47499151e-01, 0.00000000e+00, 2.86424926e-01, 0.00000000e+00]\n",
    "\n",
    "a1 = np.float64(reference_params[0])\n",
    "a2 = np.float64(reference_params[1])\n",
    "a3 = np.float64(reference_params[2])\n",
    "a4 = np.float64(reference_params[3])\n",
    "a5 = np.float64(reference_params[4])\n",
    "a6 = np.float64(reference_params[5])\n",
    "\n",
    "\n",
    "data=np.array(data2)\n",
    "\n",
    "# print('data =', data)\n",
    "\n",
    "for tuple in optimal_params_vec:\n",
    "    \n",
    "    optimal_params = tuple\n",
    "\n",
    "    print(\"Reference parameters:\", reference_params)\n",
    "    \n",
    "    # Imprimir os parâmetros otimizados\n",
    "    print(\"Optimal parameters:\", optimal_params)   \n",
    "    \n",
    "    # referencia\n",
    "    print('reference (special case)')\n",
    "    aic, aicc, bic, edc, logLik = eval_fit_extreme(data, [a1, a2, a3, a4, a5, a6])\n",
    "    \n",
    "    print('optimized')\n",
    "    aic, aicc, bic, edc, logLik = eval_fit_extreme(data, optimal_params)\n",
    "\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
