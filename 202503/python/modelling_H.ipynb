{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiago/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "# LIbraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import weibull_min,gamma, gengamma, invgamma,halfnorm,halfgennorm,rayleigh,erlang\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import genextreme\n",
    "from scipy import integrate\n",
    "from pynverse import inversefunc\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import uniform\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import root_scalar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def H_teta(a1, a2, a3, a4, a5, a6):\n",
    "    return quad(lambda x: (x**a6)*np.exp(-a1*x - (a2*x**a3 + a4)**a5), 0, np.inf)[0]\n",
    "\n",
    "def pdf_HG(y, par):\n",
    "    t1, t2, t3, t4, t5, t6 = par\n",
    "    H_val = H_teta(t1, t2, t3, t4, t5, t6)\n",
    "    y = np.array(y)\n",
    "    pdfh = 1 / H_val * y**t6 * np.exp(-t1 * y - (t2 * y**t3 + t4)**t5)\n",
    "    return pdfh\n",
    "\n",
    "def NumIntH(a1, a2, a3, a4, a5, a6):\n",
    "    return quad(lambda x: (x**a6)*np.exp(-a1*x - (a2*x**a3 + a4)**a5), 0, np.inf)[0]\n",
    "\n",
    "def NumIntH_distribution(a1, a2, a3, a4, a5, a6, y):\n",
    "    return quad(lambda x: (x**a6)*np.exp(-a1*x - (a2*x**a3 + a4)**a5), 0, y)[0]\n",
    "\n",
    "def distribution_extreme_H(x, par):\n",
    "    t1, t2, t3, t4, t5, t6 = par\n",
    "    c_theta = NumIntH(t1, t2, t3, t4, t5, t6)\n",
    "   \n",
    "    cdf_vec = [0.]*len(x)\n",
    "    \n",
    "    for index, val in enumerate(x):\n",
    "        cdf_vec[index] = (1./c_theta) * NumIntH_distribution(t1, t2, t3, t4, t5, t6, val)\n",
    "\n",
    "    return cdf_vec\n",
    "\n",
    "def ecdf(data):\n",
    "    x = np.sort(data)\n",
    "    y = np.arange(1, len(data) + 1) / len(data)\n",
    "    return x, y\n",
    "    \n",
    "# cost/loss function\n",
    "def ecdf_error(params, data):\n",
    "    x_empirical, y_empirical = ecdf(data)\n",
    "    y_theoretical = distribution_extreme_H(x_empirical, params)\n",
    "    error = np.sum((y_empirical - y_theoretical) ** 2)\n",
    "    return error\n",
    "\n",
    "def distribution_extreme_H_ks(x):\n",
    "    t1, t2, t3, t4, t5, t6 = parGlobal\n",
    "    c_theta = NumIntH(t1, t2, t3, t4, t5, t6)\n",
    "   \n",
    "    cdf_vec = [0.]*len(x)\n",
    "    \n",
    "    for index, val in enumerate(x):\n",
    "        if val==0:\n",
    "            cdf_vec[index] = 0\n",
    "        elif np.isnan(val):\n",
    "            cdf_vec[index] = 0\n",
    "        else:\n",
    "            # print('val =', val)\n",
    "            cdf_vec[index] = (1./c_theta) * NumIntH_distribution(t1, t2, t3, t4, t5, t6, val)\n",
    "\n",
    "    return cdf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#density function\n",
    "#y: data set\n",
    "def dg(y, theta1, theta2, theta3, theta4, theta5, theta6):\n",
    "    # print('DG theta1=', theta1, type(theta1))\n",
    "    # print('DG y=', y, type(y))\n",
    "    c_theta = NumIntH(theta1, theta2, theta3, theta4, theta5, theta6)\n",
    "    return (1./c_theta)*(y**theta6)*np.exp(-theta1*y-((theta2*y**theta3)+theta4)**theta5)\n",
    "\n",
    "#log-likelihood function\n",
    "#x0: data set\n",
    "def llf(x, t1, t2, t3, t4, t5, t6):\n",
    "\n",
    "    # print('llf pre-filter x=', x)\n",
    "\n",
    "    #eliminar os pontos com densidade zero, para podermos aplicar o log\n",
    "    x0=x[dg(x, t1, t2, t3, t4, t5, t6)>0]\n",
    "\n",
    "    # print('llf pos-filter x0=', x0, 'x<=0: ', x[dg(x, t1, t2, t3, t4, t5, t6)<=0])\n",
    "\n",
    "    ##TENTAR FAZER VETORIZADO\n",
    "    return (sum(np.log(dg(x0, t1, t2, t3, t4, t5, t6))))\n",
    "\n",
    "def minus_llf(params, data):\n",
    "    t1, t2, t3, t4, t5, t6 = params\n",
    "    return -1*llf(data, t1, t2, t3, t4, t5, t6)\n",
    "    \n",
    "def eval_fit_extreme(samples, parameters_tuple, no_print=True):\n",
    "      \n",
    "    logLik = llf(samples, parameters_tuple[0], parameters_tuple[1], parameters_tuple[2], parameters_tuple[3], parameters_tuple[4], parameters_tuple[5])\n",
    "\n",
    "    k = len(parameters_tuple)\n",
    "    aic = 2*k - 2*(logLik)\n",
    "    n = np.log(len(samples))\n",
    "    aicc = aic + 2*k*(k+1)/(n-k-1)\n",
    "\n",
    "    bic = k*n-2*logLik\n",
    "    edc = -2*logLik-k*np.log(n)\n",
    "\n",
    "    if(no_print):\n",
    "        print('AIC: ', aic, 'AICc:', aicc, 'BIC: ', bic, 'EDC:', edc,'Log-Likelihood', logLik)\n",
    "\n",
    "    parGlobal = parameters_tuple\n",
    "    \n",
    "    ks = stats.kstest(samples, distribution_extreme_H_ks)\n",
    "\n",
    "    if(no_print):\n",
    "        print('KS :', ks)\n",
    "    \n",
    "    return aic, aicc, bic, edc, logLik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frechet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y  (np.float64(1.5644948551004678), 0, np.float64(13.76067644260458))\n"
     ]
    }
   ],
   "source": [
    "# Piracicaba River dataset\n",
    "\n",
    "september_flow = [29.19, 8.49, 7.37, 82.93, 44.18, 13.82, 22.28, 28.06, 6.84, 12.14, 153.78, 17.04, 13.47, 15.43, 30.36, 6.91, 22.12, 35.45, 44.66, 95.81, 6.18, 10.00, 58.39, 24.05, 17.03, 38.65, 47.17, 27.99, 11.84, 9.60, 6.72, 13.74, 14.60, 9.65, 10.39, 60.14, 15.51, 14.69, 16.44]\n",
    "\n",
    "\n",
    "# Frechet\n",
    "base_distribution = stats.invweibull\n",
    "\n",
    "Y = september_flow\n",
    "Y_parameters = base_distribution.fit(Y, floc=0)\n",
    "\n",
    "\n",
    "print('Y ', Y_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29.19, 8.49, 7.37, 82.93, 44.18, 13.82, 22.28, 28.06, 6.84, 12.14, 153.78, 17.04, 13.47, 15.43, 30.36, 6.91, 22.12, 35.45, 44.66, 95.81, 6.18, 10.0, 58.39, 24.05, 17.03, 38.65, 47.17, 27.99, 11.84, 9.6, 6.72, 13.74, 14.6, 9.65, 10.39, 60.14, 15.51, 14.69, 16.44]\n",
      "shape = 1.5644948551004678  scale= 13.76067644260458\n",
      "initial guess  [0, np.float64(0.07267084610055136), 1, 0, np.float64(-1.5644948551004678), np.float64(-2.5644948551004676)]\n",
      "ECDF optimzed parameters: [ 1.30923457e-02  3.63820093e-01  5.16540683e-01  6.02528066e-04\n",
      " -5.08883272e+00 -1.44633923e+00]\n",
      "MLE Optimized Parameters: [ 2.10280344e-02  7.67695842e-04  3.97317286e+00  7.11604099e-03\n",
      " -9.50968970e+00 -8.99142058e-01]\n"
     ]
    }
   ],
   "source": [
    "bounds = [(0, 10), (0, 10), (-10, 10), (0, 10), (-10, 10), (-10, 50)]\n",
    "\n",
    "#(beta, 0, 1, 0, 1, alpha − 1)\n",
    "#- Alpha (shape): 1.02936544721869\n",
    "#- Beta (rate): 0.005199497909196325\n",
    "\n",
    "# sigma = X_parameters[2]\n",
    "# shape = X_parameters[0]\n",
    "# initial_params = [0, 1/sigma, 1, 0, -shape, -shape-1]\n",
    "\n",
    "# print(X)\n",
    "\n",
    "sigma = Y_parameters[2]\n",
    "shape = Y_parameters[0]\n",
    "initial_params = [0, 1/sigma, 1, 0, -shape, -shape-1]\n",
    "\n",
    "print(Y)\n",
    "\n",
    "print('shape =', shape, ' scale=', sigma)\n",
    "print('initial guess ', initial_params)\n",
    "\n",
    "# Minimização da diferença entre ECDF empírica e CDF teórica\n",
    "# result = minimize(ecdf_error, initial_params, args=(np.asarray(Y),), bounds=bounds, method='L-BFGS-B')\n",
    "# result = minimize(ecdf_error, initial_params, args=(X,), bounds=bounds, method='L-BFGS-B')\n",
    "result = minimize(ecdf_error, initial_params, args=(Y,), bounds=bounds, method='Nelder-Mead')\n",
    "optimal_params = result.x\n",
    "\n",
    "# Imprimir os parâmetros otimizados\n",
    "print(\"ECDF optimzed parameters:\", optimal_params)\n",
    "\n",
    "\n",
    "# initial_params_opt = optimal_params\n",
    "# a run from hyperopt\n",
    "# initial_params_opt = [np.float64(0.009991074400381036), np.float64(9.183842704716081), np.float64(-1.1564844738808813), np.float64(0.007812904290153418), np.float64(3.7633790172371717), np.float64(-1.4877563479225495)]\n",
    "\n",
    "\n",
    "# Minimização da llf\n",
    "# bounds = [(0, 10), (0, 10), (-10, 50), (0, 10), (-10, 50), (-10, 50)]\n",
    "result = minimize(minus_llf, optimal_params, args=(np.asarray(Y),), bounds=bounds, method='Nelder-Mead')\n",
    "# result = minimize(minus_llf, initial_params_opt, args=(np.asarray(Y),), bounds=bounds, method='L-BFGS-B')\n",
    "# bounds = [(0, 10), (0, 10), (-10, 10), (0, 10), (-10, 10), (-10, 50)]\n",
    "# result = minimize(minus_llf, initial_params_opt, args=(np.asarray(Y),), bounds=bounds, method='Nelder-Mead')\n",
    "optimal_params_llf = result.x\n",
    "\n",
    "print(\"MLE Optimized Parameters:\", optimal_params_llf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data = [ 29.19   8.49   7.37  82.93  44.18  13.82  22.28  28.06   6.84  12.14\n",
      " 153.78  17.04  13.47  15.43  30.36   6.91  22.12  35.45  44.66  95.81\n",
      "   6.18  10.    58.39  24.05  17.03  38.65  47.17  27.99  11.84   9.6\n",
      "   6.72  13.74  14.6    9.65  10.39  60.14  15.51  14.69  16.44]\n",
      "Reference parameters: [0, 0.07267084610055136, 1, 0, -1.5644948551004678, -2.5644948551004676]\n",
      "Optimal parameters: [0.0130923457, 0.363820093, 0.516540683, 0.000602528066, -5.08883272, -1.44633923]\n",
      "reference\n",
      "AIC:  333.69480197665627 AICc: 308.51825408613166 BIC:  343.67617185343414 EDC: 313.9041871671185 Log-Likelihood -160.84740098832813\n",
      "KS : KstestResult(statistic=np.float64(0.07841473668358723), pvalue=np.float64(0.9548579573017629), statistic_location=np.float64(27.99), statistic_sign=np.int8(-1))\n",
      "fitted\n",
      "AIC:  332.33626799893693 AICc: 307.15972010841233 BIC:  342.3176378757148 EDC: 312.54565318939916 Log-Likelihood -160.16813399946847\n",
      "KS : KstestResult(statistic=np.float64(0.07175396740489254), pvalue=np.float64(0.9793882843819256), statistic_location=np.float64(13.47), statistic_sign=np.int8(-1))\n",
      "\n",
      "\n",
      "Reference parameters: [0, 0.07267084610055136, 1, 0, -1.5644948551004678, -2.5644948551004676]\n",
      "Optimal parameters: [0.0210280344, 0.000767695842, 3.97317286, 0.00711604099, -9.5096897, -0.899142058]\n",
      "reference\n",
      "AIC:  333.69480197665627 AICc: 308.51825408613166 BIC:  343.67617185343414 EDC: 313.9041871671185 Log-Likelihood -160.84740098832813\n",
      "KS : KstestResult(statistic=np.float64(0.07841473668358723), pvalue=np.float64(0.9548579573017629), statistic_location=np.float64(27.99), statistic_sign=np.int8(-1))\n",
      "fitted\n",
      "AIC:  328.89213306506207 AICc: 303.71558517453747 BIC:  338.87350294183994 EDC: 309.1015182555243 Log-Likelihood -158.44606653253103\n",
      "KS : KstestResult(statistic=np.float64(0.08515066068903276), pvalue=np.float64(0.9171237046531593), statistic_location=np.float64(17.04), statistic_sign=np.int8(1))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_parameters = [1.5644948551004678,  0, 13.76067644260458]\n",
    "shape = Y_parameters[0]\n",
    "sigma = Y_parameters[2]\n",
    "reference_params = [0, 1/sigma, 1, 0, -shape, -shape-1]\n",
    "\n",
    "optimal_params_vec = [ [ 1.30923457e-02,  3.63820093e-01,  5.16540683e-01,  6.02528066e-04, -5.08883272e+00, -1.44633923e+00],\n",
    "                      [ 2.10280344e-02,  7.67695842e-04,  3.97317286e+00,  7.11604099e-03, -9.50968970e+00, -8.99142058e-01]  ]\n",
    "\n",
    "a1 = np.float64(reference_params[0])\n",
    "a2 = np.float64(reference_params[1])\n",
    "a3 = np.float64(reference_params[2])\n",
    "a4 = np.float64(reference_params[3])\n",
    "a5 = np.float64(reference_params[4])\n",
    "a6 = np.float64(reference_params[5])\n",
    "\n",
    "data2 = Y\n",
    "\n",
    "\n",
    "data=np.array(data2)\n",
    "\n",
    "print('data =', data)\n",
    "\n",
    "for tuple in optimal_params_vec:\n",
    "    \n",
    "    optimal_params = tuple\n",
    "\n",
    "    print(\"Reference parameters:\", reference_params)\n",
    "    \n",
    "    # Imprimir os parâmetros otimizados\n",
    "    print(\"Optimal parameters:\", optimal_params)       \n",
    "    \n",
    "    # referencia\n",
    "    print('reference')\n",
    "    parGlobal = reference_params\n",
    "    aic, aicc, bic, edc, logLik = eval_fit_extreme(data, [a1, a2, a3, a4, a5, a6])\n",
    "    \n",
    "    print('fitted')\n",
    "    parGlobal = optimal_params\n",
    "    aic, aicc, bic, edc, logLik = eval_fit_extreme(data, optimal_params)\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weilbull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X  (np.float64(5.504860090269867), 0, np.float64(2.650855606967049))\n"
     ]
    }
   ],
   "source": [
    "# The data represent the strength data measured in GPA, for single carbon fibers. Single fibers were tested under tension at gauge length of 20 mm. 69 samples\n",
    "X_carbon_fiber = [1.312,1.314,1.479,1.552,1.700,1.803,1.861,1.865,1.944,1.958,1.966,1.997,2.006,2.021,2.027,2.055, 2.063,2.098,2.140,2.179,2.224,2.240,2.253,2.270,2.272,2.274,2.301,2.301,2.359,2.382,2.382,2.426, 2.434,2.435,2.478,2.490,2.511,2.514,2.535,2.554,2.566,2.570,2.586,2.629,2.633,2.642,2.648,2.684, 2.697,2.726,2.770,2.773,2.800,2.809,2.818,2.821,2.848,2.880,2.954,3.012,3.067,3.084,3.090,3.096, 3.128,3.233,3.433,3.585,3.585]\n",
    "\n",
    "base_distribution = stats.weibull_min\n",
    "\n",
    "# Validado contra: [Nojosa and Rathie, 2020] Nojosa, R. and Rathie, P. (2020). Stress–strength reliability models involving generalized gamma and Weibull distributions. International Journal of Quality & Reliability Management, 37(4):538–551.\n",
    "X = X_carbon_fiber\n",
    "\n",
    "X_parameters = base_distribution.fit(X, floc=0)\n",
    "\n",
    "print('X ', X_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.float64(5.504860090269867), 0, np.float64(2.650855606967049))\n",
      "X & 1.312  &  2.098  &  2.478  &  2.4513333333333334  &  2.773  &  3.585  &  69\n"
     ]
    }
   ],
   "source": [
    "print(X_parameters)\n",
    "print('X &', min(X),' & ', np.percentile(X,25), ' & ', np.median(X), ' & ', np.mean(X), ' & ', np.percentile(X,75), ' & ', max(X), ' & ', len(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.312, 1.314, 1.479, 1.552, 1.7, 1.803, 1.861, 1.865, 1.944, 1.958, 1.966, 1.997, 2.006, 2.021, 2.027, 2.055, 2.063, 2.098, 2.14, 2.179, 2.224, 2.24, 2.253, 2.27, 2.272, 2.274, 2.301, 2.301, 2.359, 2.382, 2.382, 2.426, 2.434, 2.435, 2.478, 2.49, 2.511, 2.514, 2.535, 2.554, 2.566, 2.57, 2.586, 2.629, 2.633, 2.642, 2.648, 2.684, 2.697, 2.726, 2.77, 2.773, 2.8, 2.809, 2.818, 2.821, 2.848, 2.88, 2.954, 3.012, 3.067, 3.084, 3.09, 3.096, 3.128, 3.233, 3.433, 3.585, 3.585]\n",
      "shape = 5.504860090269867  scale= 2.650855606967049\n",
      "initial guess  [0, np.float64(0.37723669194646947), 1, 0, np.float64(5.504860090269867), np.float64(4.504860090269867)]\n",
      "ECDF Optimized parameters: [2.22412378e-03 4.99892765e-01 8.03443114e-01 3.12033933e-02\n",
      " 5.43824247e+00 6.12402375e+00]\n",
      "MLE Optimized parameters: [2.55584476e-05 6.43480671e-01 6.27314530e-01 1.83895256e-02\n",
      " 5.40854354e+00 7.15147852e+00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80691/3474878172.py:12: IntegrationWarning: The algorithm does not converge.  Roundoff error is detected\n",
      "  in the extrapolation table.  It is assumed that the requested tolerance\n",
      "  cannot be achieved, and that the returned result (if full_output = 1) is \n",
      "  the best which can be obtained.\n",
      "  return quad(lambda x: (x**a6)*np.exp(-a1*x - (a2*x**a3 + a4)**a5), 0, np.inf)[0]\n"
     ]
    }
   ],
   "source": [
    "bounds = [(0, 10), (0, 10), (-10, 10), (0, 10), (-10, 10), (0, 50)]\n",
    "\n",
    "\n",
    "shape = X_parameters[0]\n",
    "sigma = X_parameters[2]\n",
    "\n",
    "# Weibull\n",
    "initial_params = [0, 1/sigma, 1, 0, shape, shape-1]\n",
    "\n",
    "print(X)\n",
    "\n",
    "print('shape =', shape, ' scale=', sigma)\n",
    "print('initial guess ', initial_params)\n",
    "\n",
    "# Plotar a ECDF empírica e a CDF teórica ajustada\n",
    "x_empirical, y_empirical = ecdf(X)\n",
    "\n",
    "\n",
    "# Minimização da diferença entre ECDF empírica e CDF teórica\n",
    "result = minimize(ecdf_error, initial_params, args=(X,), bounds=bounds, method='L-BFGS-B')\n",
    "# result = minimize(ecdf_error, initial_params, args=(Y,), bounds=bounds, method='L-BFGS-B')\n",
    "optimal_params = result.x\n",
    "\n",
    "# Imprimir os parâmetros otimizados\n",
    "print(\"ECDF Optimized parameters:\", optimal_params)\n",
    "\n",
    "\n",
    "# Minimizing LLF\n",
    "# result = minimize(minus_llf, initial_params, args=(np.asarray(X),), bounds=bounds, method='L-BFGS-B')\n",
    "result = minimize(minus_llf, optimal_params, args=(np.asarray(X),), bounds=bounds, method='L-BFGS-B')\n",
    "optimal_params_llf = result.x\n",
    "\n",
    "print(\"MLE Optimized parameters:\", optimal_params_llf)\n",
    "\n",
    "y_theoretical = distribution_extreme_H(x_empirical, optimal_params)\n",
    "y_theoretical_llf = distribution_extreme_H(x_empirical, optimal_params_llf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data = [1.312 1.314 1.479 1.552 1.7   1.803 1.861 1.865 1.944 1.958 1.966 1.997\n",
      " 2.006 2.021 2.027 2.055 2.063 2.098 2.14  2.179 2.224 2.24  2.253 2.27\n",
      " 2.272 2.274 2.301 2.301 2.359 2.382 2.382 2.426 2.434 2.435 2.478 2.49\n",
      " 2.511 2.514 2.535 2.554 2.566 2.57  2.586 2.629 2.633 2.642 2.648 2.684\n",
      " 2.697 2.726 2.77  2.773 2.8   2.809 2.818 2.821 2.848 2.88  2.954 3.012\n",
      " 3.067 3.084 3.09  3.096 3.128 3.233 3.433 3.585 3.585]\n",
      "Reference parameters: [0, np.float64(0.37723669194646947), 1, 0, np.float64(5.504860090269867), np.float64(4.504860090269867)]\n",
      "Optimal parameters: [2.22412378e-03 4.99892765e-01 8.03443114e-01 3.12033933e-02\n",
      " 5.43824247e+00 6.12402375e+00]\n",
      "reference\n",
      "AIC:  111.1922702651877 AICc: 80.82233731599119 BIC:  124.59690929277126 EDC: 90.53323630318033 Log-Likelihood -49.59613513259385\n",
      "KS : KstestResult(statistic=np.float64(0.056129327685893515), pvalue=np.float64(0.9730632666899052), statistic_location=np.float64(2.821), statistic_sign=np.int8(1))\n",
      "fitted\n",
      "AIC:  110.34667430479332 AICc: 79.97674135559681 BIC:  123.75131333237688 EDC: 89.68764034278595 Log-Likelihood -49.17333715239666\n",
      "KS : KstestResult(statistic=np.float64(0.04167065644897927), pvalue=np.float64(0.9993883568390188), statistic_location=np.float64(3.067), statistic_sign=np.int8(-1))\n",
      "\n",
      "\n",
      "Reference parameters: [0, np.float64(0.37723669194646947), 1, 0, np.float64(5.504860090269867), np.float64(4.504860090269867)]\n",
      "Optimal parameters: [2.55584476e-05 6.43480671e-01 6.27314530e-01 1.83895256e-02\n",
      " 5.40854354e+00 7.15147852e+00]\n",
      "reference\n",
      "AIC:  111.1922702651877 AICc: 80.82233731599119 BIC:  124.59690929277126 EDC: 90.53323630318033 Log-Likelihood -49.59613513259385\n",
      "KS : KstestResult(statistic=np.float64(0.056129327685893515), pvalue=np.float64(0.9730632666899052), statistic_location=np.float64(2.821), statistic_sign=np.int8(1))\n",
      "fitted\n",
      "AIC:  109.74578378863751 AICc: 79.375850839441 BIC:  123.15042281622107 EDC: 89.08674982663014 Log-Likelihood -48.872891894318755\n",
      "KS : KstestResult(statistic=np.float64(0.040509546600778656), pvalue=np.float64(0.9996260210974043), statistic_location=np.float64(2.821), statistic_sign=np.int8(1))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reference_params =  [0, np.float64(0.37723669194646947), 1, 0, np.float64(5.504860090269867), np.float64(4.504860090269867)]\n",
    "\n",
    "# optimized\n",
    "optimal_params_vec = [ optimal_params,\n",
    "                     optimal_params_llf]\n",
    "\n",
    "a1 = np.float64(reference_params[0])\n",
    "a2 = np.float64(reference_params[1])\n",
    "a3 = np.float64(reference_params[2])\n",
    "a4 = np.float64(reference_params[3])\n",
    "a5 = np.float64(reference_params[4])\n",
    "a6 = np.float64(reference_params[5])\n",
    "\n",
    "data2 = X\n",
    "\n",
    "\n",
    "data=np.array(data2)\n",
    "\n",
    "print('data =', data)\n",
    "\n",
    "for tuple in optimal_params_vec:\n",
    "    \n",
    "    optimal_params = tuple\n",
    "\n",
    "    print(\"Reference parameters:\", reference_params)\n",
    "    \n",
    "    # Imprimir os parâmetros otimizados\n",
    "    print(\"Optimal parameters:\", optimal_params)       \n",
    "    \n",
    "    # referencia\n",
    "    print('reference')\n",
    "    parGlobal = reference_params\n",
    "    aic, aicc, bic, edc, logLik = eval_fit_extreme(data, [a1, a2, a3, a4, a5, a6])\n",
    "    \n",
    "    print('fitted')\n",
    "    parGlobal = optimal_params\n",
    "    aic, aicc, bic, edc, logLik = eval_fit_extreme(data, optimal_params)\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "[620, 470, 260, 89, 388, 242, 103, 100, 39, 460, 284, 1285, 218, 393, 106, 158, 152, 477, 403, 103, 69, 158, 818, 947, 399, 1274, 32, 12, 134, 660, 548, 381, 203, 871, 193, 531, 317, 85, 1410, 250, 41, 1101, 32, 421, 32, 343, 376, 1512, 1792, 47, 95, 76, 515, 72, 1585, 253, 6, 860, 89, 1055, 537, 101, 385, 176, 11, 565, 164, 16, 1267, 352, 160, 195, 1279, 356, 751, 500, 803, 560, 151, 24, 689, 1119, 1733, 2194, 763, 555, 14, 45, 776, 1, 1747, 945, 12, 1453, 14, 150, 20, 41, 35, 69, 195, 89, 1090, 1868, 294, 96, 618, 44, 142, 892, 1307, 310, 230, 30, 403, 860, 23, 406, 1054, 1935, 561, 348, 130, 13, 230, 250, 317, 304, 79, 1793, 536, 12, 9, 256, 201, 733, 510, 660, 122, 27, 273, 1231, 182, 289, 667, 761, 1096, 43, 44, 87, 405, 998, 1409, 61, 278, 407, 113, 25, 940, 28, 848, 41, 646, 575, 219, 303, 304, 38, 195, 1061, 174, 377, 388, 10, 246, 323, 198, 234, 39, 308, 55, 729, 813, 1216, 1618, 539, 6, 1566, 459, 946, 764, 794, 35, 181, 147, 116, 141, 19, 380, 609, 546]\n",
      "Z & 1.312  &  2.098  &  2.478  &  2.4513333333333334  &  2.773  &  3.585  &  69\n"
     ]
    }
   ],
   "source": [
    "Z = [620,470,260,89,388,242, 103,100,39,460,284,1285, 218,393,106,158,152,477, 403,103,69,158,818,947, 399,1274,32,12,134,660, 548,381,203,871,193,531, 317,85,1410,250,41,1101, 32,421,32,343,376,1512, 1792,47,95,76,515,72, 1585,253,6,860,89,1055, 537,101,385,176,11,565, 164,16,1267,352,160,195, 1279,356,751,500,803,560, 151,24,689,1119,1733,2194, 763,555,14,45,776,1, 1747,945,12,1453,14,150, 20,41,35,69,195,89, 1090,1868,294,96,618,44, 142,892,1307,310,230,30, 403,860,23,406,1054,1935, 561,348,130,13,230,250, 317,304,79,1793,536,12, 9,256,201,733,510,660, 122,27,273,1231,182,289, 667,761,1096,43,44,87, 405,998,1409,61,278,407, 113,25,940,28,848,41, 646,575,219,303,304,38, 195,1061,174,377,388,10, 246,323,198,234,39,308, 55,729,813,1216,1618,539, 6,1566,459,946,764,794, 35,181,147,116,141,19, 380,609,546]\n",
    "\n",
    "print(len(Z))\n",
    "print(Z)\n",
    "data2 = Z\n",
    "\n",
    "print('Z &', min(X),' & ', np.percentile(X,25), ' & ', np.median(X), ' & ', np.mean(X), ' & ', np.percentile(X,75), ' & ', max(X), ' & ', len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial guess for MLE (from hyperopt) [np.float64(0.0014600180926046908), np.float64(7.168558250732656), np.float64(-6.554152619274005), np.float64(0.0017861406540565223), np.float64(8.57484235581811), np.float64(-0.29244892768030395)]\n",
      "MLE Optimized:  [ 1.74250452e-03  7.16891521e+00 -6.55335632e+00  4.56743472e-03\n",
      "  8.57482224e+00 -1.94769801e-01]\n"
     ]
    }
   ],
   "source": [
    "bounds = [(0, 10), (0, 10), (-10, 10), (0, 10), (-10, 10), (-10, 50)]\n",
    "\n",
    "shape, loc, scale = gamma.fit(data2, floc=0)  # floc=0 fixa o parâmetro de localização em 0\n",
    "\n",
    "alpha = shape\n",
    "beta =  1 / scale  \n",
    "\n",
    "#(beta, 0, 1, 0, 1, alpha − 1)\n",
    "initial_params = [beta, 0, 1, 0, 1, shape-1]\n",
    "\n",
    "initial_params_opt = [np.float64(0.0014600180926046908), np.float64(7.168558250732656), np.float64(-6.554152619274005), np.float64(0.0017861406540565223), np.float64(8.57484235581811), np.float64(-0.29244892768030395)]\n",
    "\n",
    "# Imprimir os parâmetros otimizados\n",
    "print('Initial guess for MLE (from hyperopt)', initial_params_opt)\n",
    "\n",
    "\n",
    "# initial_params_opt = initial_params\n",
    "\n",
    "# Minimização da llf\n",
    "# result = minimize(minus_llf, initial_params_opt, args=(np.asarray(data2),), bounds=bounds, method='Nelder-Mead')\n",
    "result = minimize(minus_llf, initial_params_opt, args=(np.asarray(data2),), bounds=bounds, method='L-BFGS-B')\n",
    "optimal_params_llf = result.x\n",
    "\n",
    "print(\"MLE Optimized: \", optimal_params_llf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [ 620  470  260   89  388  242  103  100   39  460  284 1285  218  393\n",
      "  106  158  152  477  403  103   69  158  818  947  399 1274   32   12\n",
      "  134  660  548  381  203  871  193  531  317   85 1410  250   41 1101\n",
      "   32  421   32  343  376 1512 1792   47   95   76  515   72 1585  253\n",
      "    6  860   89 1055  537  101  385  176   11  565  164   16 1267  352\n",
      "  160  195 1279  356  751  500  803  560  151   24  689 1119 1733 2194\n",
      "  763  555   14   45  776    1 1747  945   12 1453   14  150   20   41\n",
      "   35   69  195   89 1090 1868  294   96  618   44  142  892 1307  310\n",
      "  230   30  403  860   23  406 1054 1935  561  348  130   13  230  250\n",
      "  317  304   79 1793  536   12    9  256  201  733  510  660  122   27\n",
      "  273 1231  182  289  667  761 1096   43   44   87  405  998 1409   61\n",
      "  278  407  113   25  940   28  848   41  646  575  219  303  304   38\n",
      "  195 1061  174  377  388   10  246  323  198  234   39  308   55  729\n",
      "  813 1216 1618  539    6 1566  459  946  764  794   35  181  147  116\n",
      "  141   19  380  609  546] \n",
      "\n",
      "Reference parameters: (special case) [np.float64(0.0017845959485496782), 0, 1, 0, 1, -0.1725778595363674]\n",
      "Optimal parameters: [np.float64(0.0014600180926046908), np.float64(7.168558250732656), np.float64(-6.554152619274005), np.float64(0.0017861406540565223), np.float64(8.57484235581811), np.float64(-0.29244892768030395)]\n",
      "reference: special case\n",
      "AIC:  2876.8516235519837 AICc: 2827.343612124952 BIC:  2896.6714530003383 EDC: 2854.84164238931 Log-Likelihood -1432.4258117759919\n",
      "KS : KstestResult(statistic=np.float64(0.05213487159985372), pvalue=np.float64(0.6262658892615101), statistic_location=np.int64(47), statistic_sign=np.int8(1))\n",
      "fitted\n",
      "AIC:  2865.101368134772 AICc: 2815.5933567077404 BIC:  2884.9211975831267 EDC: 2843.0913869720985 Log-Likelihood -1426.550684067386\n",
      "KS : KstestResult(statistic=np.float64(0.03508638477637063), pvalue=np.float64(0.9582062871989735), statistic_location=np.int64(47), statistic_sign=np.int8(1))\n",
      "\n",
      "\n",
      "Reference parameters: (special case) [np.float64(0.0017845959485496782), 0, 1, 0, 1, -0.1725778595363674]\n",
      "Optimal parameters: [0.00174250452, 7.16891521, -6.55335632, 0.00456743472, 8.57482224, -0.194769801]\n",
      "reference: special case\n",
      "AIC:  2876.8516235519837 AICc: 2827.343612124952 BIC:  2896.6714530003383 EDC: 2854.84164238931 Log-Likelihood -1432.4258117759919\n",
      "KS : KstestResult(statistic=np.float64(0.05213487159985372), pvalue=np.float64(0.6262658892615101), statistic_location=np.int64(47), statistic_sign=np.int8(1))\n",
      "fitted\n",
      "AIC:  2863.103664380029 AICc: 2813.595652952997 BIC:  2882.9234938283835 EDC: 2841.093683217355 Log-Likelihood -1425.5518321900145\n",
      "KS : KstestResult(statistic=np.float64(0.05341279220140724), pvalue=np.float64(0.5958460363941905), statistic_location=np.int64(47), statistic_sign=np.int8(1))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from Gamma\n",
    "\n",
    "# shape= 0.8274221404636326\n",
    "# beta= 0.0017845959485496782\n",
    "\n",
    "reference_params = [beta, 0, 1, 0, 1, shape-1]\n",
    "\n",
    "# optimized\n",
    "optimal_params_vec = [ [np.float64(0.0014600180926046908), np.float64(7.168558250732656), np.float64(-6.554152619274005), np.float64(0.0017861406540565223), np.float64(8.57484235581811), np.float64(-0.29244892768030395)],\n",
    "                     [ 1.74250452e-03,  7.16891521e+00, -6.55335632e+00,  4.56743472e-03,  8.57482224e+00, -1.94769801e-01]]\n",
    "\n",
    "a1 = np.float64(reference_params[0])\n",
    "a2 = np.float64(reference_params[1])\n",
    "a3 = np.float64(reference_params[2])\n",
    "a4 = np.float64(reference_params[3])\n",
    "a5 = np.float64(reference_params[4])\n",
    "a6 = np.float64(reference_params[5])\n",
    "\n",
    "\n",
    "data=np.array(data2)\n",
    "\n",
    "print('Z =', data, '\\n')\n",
    "\n",
    "for tuple in optimal_params_vec:\n",
    "    \n",
    "    optimal_params = tuple\n",
    "\n",
    "    print(\"Reference parameters: (special case)\", reference_params)\n",
    "    \n",
    "    # Imprimir os parâmetros otimizados\n",
    "    print(\"Optimal parameters:\", optimal_params)\n",
    "    \n",
    "    \n",
    "    print('reference: special case')\n",
    "    parGlobal = [a1, a2, a3, a4, a5, a6]\n",
    "    aic, aicc, bic, edc, logLik = eval_fit_extreme(data, [a1, a2, a3, a4, a5, a6])\n",
    "    \n",
    "    print('fitted')\n",
    "    parGlobal = optimal_params\n",
    "    aic, aicc, bic, edc, logLik = eval_fit_extreme(data, optimal_params)\n",
    "\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
